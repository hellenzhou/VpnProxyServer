#include "worker_thread_pool.h"
#include "packet_forwarder.h"
#include "udp_retransmit.h"
#include "vpn_server_globals.h"
#include "packet_builder.h"
#include "protocol_handler.h"
#include <hilog/log.h>
#include <sys/socket.h>
#include <arpa/inet.h>
#include <thread>
#include <chrono>

#define MAKE_FILE_NAME (strrchr(__FILE__, '/') ? (strrchr(__FILE__, '/') + 1) : __FILE__)
#define WORKER_LOGI(fmt, ...) \
  OH_LOG_Print(LOG_APP, LOG_INFO, 0x15b1, "VpnServer", "ZBQ [Worker] [%{public}s:%{public}d] " fmt, MAKE_FILE_NAME, __LINE__, ##__VA_ARGS__)
#define WORKER_LOGE(fmt, ...) \
  OH_LOG_Print(LOG_APP, LOG_ERROR, 0x15b1, "VpnServer", "ZBQ [Worker] [%{public}s:%{public}d] " fmt, MAKE_FILE_NAME, __LINE__, ##__VA_ARGS__)

bool WorkerThreadPool::start(int numForwardWorkers, int numResponseWorkers) {
    WORKER_LOGI("ğŸ“ WorkerThreadPool::start() called - numForward=%d, numResponse=%d", 
                numForwardWorkers, numResponseWorkers);
    WORKER_LOGI("ğŸ“ Current state: running_=%d, forwardWorkers.size=%zu, responseWorkers.size=%zu",
                running_.load() ? 1 : 0, forwardWorkers_.size(), responseWorkers_.size());
    
    if (running_.load()) {
        WORKER_LOGE("âš ï¸ Worker thread pool already running - cannot start again!");
        return false;
    }
    
    WORKER_LOGI("ğŸ“ Setting running_ to true...");
    running_.store(true);
    
    WORKER_LOGI("ğŸ“ Starting %d forward worker threads...", numForwardWorkers);
    // å¯åŠ¨è½¬å‘å·¥ä½œçº¿ç¨‹
    for (int i = 0; i < numForwardWorkers; ++i) {
        WORKER_LOGI("ğŸ“ Creating forward worker #%d...", i);
        forwardWorkers_.emplace_back([this, i]() {
            WORKER_LOGI("ğŸš€ Forward worker #%{public}d thread STARTED (running_=%d)", i, running_.load() ? 1 : 0);
            forwardWorkerThread();
            WORKER_LOGI("ğŸ”š Forward worker #%{public}d thread STOPPED", i);
        });
    }
    WORKER_LOGI("âœ… %d forward workers created", numForwardWorkers);
    
    WORKER_LOGI("ğŸ“ Starting %d response worker threads...", numResponseWorkers);
    // å¯åŠ¨å“åº”å·¥ä½œçº¿ç¨‹
    for (int i = 0; i < numResponseWorkers; ++i) {
        WORKER_LOGI("ğŸ“ Creating response worker #%d...", i);
        responseWorkers_.emplace_back([this, i]() {
            WORKER_LOGI("ğŸš€ Response worker #%{public}d thread STARTED (running_=%d)", i, running_.load() ? 1 : 0);
            responseWorkerThread();
            WORKER_LOGI("ğŸ”š Response worker #%{public}d thread STOPPED", i);
        });
    }
    WORKER_LOGI("âœ… %d response workers created", numResponseWorkers);
    
    WORKER_LOGI("âœ…âœ…âœ… Worker thread pool FULLY started: %{public}d forward workers, %{public}d response workers",
                numForwardWorkers, numResponseWorkers);
    
    // ç»™çº¿ç¨‹ä¸€ç‚¹æ—¶é—´å¯åŠ¨
    std::this_thread::sleep_for(std::chrono::milliseconds(50));
    
    WORKER_LOGI("ğŸ“ Final state: running_=%d", running_.load() ? 1 : 0);
    
    return true;
}

void WorkerThreadPool::stop() {
    if (!running_.load()) {
        return;
    }
    
    WORKER_LOGI("ğŸ›‘ Stopping worker thread pool...");
    running_.store(false);
    
    // å…³é—­ä»»åŠ¡é˜Ÿåˆ—ï¼Œå”¤é†’æ‰€æœ‰ç­‰å¾…çš„çº¿ç¨‹
    TaskQueueManager::getInstance().shutdown();
    
    // ç­‰å¾…æ‰€æœ‰è½¬å‘å·¥ä½œçº¿ç¨‹ç»“æŸ
    for (auto& worker : forwardWorkers_) {
        if (worker.joinable()) {
            worker.join();
        }
    }
    forwardWorkers_.clear();
    
    // ç­‰å¾…æ‰€æœ‰å“åº”å·¥ä½œçº¿ç¨‹ç»“æŸ
    for (auto& worker : responseWorkers_) {
        if (worker.joinable()) {
            worker.join();
        }
    }
    responseWorkers_.clear();
    
    WORKER_LOGI("âœ… Worker thread pool stopped");
}

void WorkerThreadPool::forwardWorkerThread() {
    auto& taskQueue = TaskQueueManager::getInstance();
    int iteration = 0;
    int processedTasks = 0;

    WORKER_LOGI("ğŸš€ğŸš€ğŸš€ Forward worker LOOP STARTED - running_=%d", running_.load() ? 1 : 0);

    while (running_.load()) {
        iteration++;
        
        // æ¯1000æ¬¡è¿­ä»£è¾“å‡ºä¸€æ¬¡å¿ƒè·³
        if (iteration % 1000 == 0) {
            WORKER_LOGI("ğŸ’“ Forward worker heartbeat: iteration=%d, processed=%d, running_=%d", 
                        iteration, processedTasks, running_.load() ? 1 : 0);
        }

        // ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼ˆ100msè¶…æ—¶ï¼‰
        auto taskOpt = taskQueue.popForwardTask(std::chrono::milliseconds(100));

        if (!taskOpt.has_value()) {
            continue;  // è¶…æ—¶æˆ–é˜Ÿåˆ—å…³é—­
        }

        // ğŸ› ä¿®å¤ï¼šå¤åˆ¶Taskå¯¹è±¡è€Œä¸æ˜¯å¼•ç”¨ï¼Œé¿å…ç”Ÿå‘½å‘¨æœŸé—®é¢˜
        Task task = taskOpt.value();
        if (task.type != TaskType::FORWARD_REQUEST) {
            WORKER_LOGE("âŒ Invalid task type in forward queue");
            continue;
        }

        ForwardTask& fwdTask = task.forwardTask;
        processedTasks++;

        // è®°å½•å‰å‡ ä¸ªä»»åŠ¡å’Œæ¯100ä¸ªä»»åŠ¡
        if (processedTasks <= 10 || processedTasks % 100 == 0) {
            WORKER_LOGI("ğŸ“Š Forward worker processing task #%{public}d: %s -> %s:%d", 
                        processedTasks,
                        ProtocolHandler::GetProtocolName(fwdTask.packetInfo.protocol).c_str(),
                        fwdTask.packetInfo.targetIP.c_str(), 
                        fwdTask.packetInfo.targetPort);
        }

        // è½¬å‘æ•°æ®åŒ…
        int sockFd = PacketForwarder::ForwardPacket(
            fwdTask.data,
            fwdTask.dataSize,
            fwdTask.packetInfo,
            fwdTask.clientAddr
        );

        if (sockFd >= 0) {
            forwardTasksProcessed_.fetch_add(1);

            // UDPåŒ…è®°å½•åˆ°é‡ä¼ ç®¡ç†å™¨ï¼ˆåªå¯¹DNSæŸ¥è¯¢ï¼‰
            if (fwdTask.packetInfo.protocol == PROTOCOL_UDP &&
                fwdTask.packetInfo.targetPort == 53) {

                uint16_t packetId = UdpRetransmitManager::generatePacketId();

                // æå–payload
                const uint8_t* payload = nullptr;
                int payloadSize = 0;
                if (PacketBuilder::ExtractPayload(fwdTask.data, fwdTask.dataSize,
                                                 fwdTask.packetInfo, &payload, &payloadSize)) {
                    if (payload && payloadSize > 0) {
                        sockaddr_in targetAddr{};
                        targetAddr.sin_family = AF_INET;
                        targetAddr.sin_port = htons(fwdTask.packetInfo.targetPort);

                        if (inet_pton(AF_INET, fwdTask.packetInfo.targetIP.c_str(), &targetAddr.sin_addr) > 0) {
                            UdpRetransmitManager::getInstance().recordSentPacket(
                                packetId, payload, payloadSize, targetAddr, sockFd);
                        }
                    }
                }
            }
        } else {
            forwardTasksFailed_.fetch_add(1);
            WORKER_LOGE("âŒ Forward task #%{public}d FAILED", processedTasks);
        }
    }

    WORKER_LOGI("ğŸ”šğŸ”šğŸ”š Forward worker LOOP STOPPED (processed %{public}d tasks, running_=%d)", 
                processedTasks, running_.load() ? 1 : 0);
}

void WorkerThreadPool::responseWorkerThread() {
    auto& taskQueue = TaskQueueManager::getInstance();
    int processedTasks = 0;

    WORKER_LOGI("ğŸš€ Response worker started");

    while (running_.load()) {
        // ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ï¼ˆ100msè¶…æ—¶ï¼‰
        auto taskOpt = taskQueue.popResponseTask(std::chrono::milliseconds(100));

        if (!taskOpt.has_value()) {
            continue;  // è¶…æ—¶æˆ–é˜Ÿåˆ—å…³é—­
        }

        // ğŸ› ä¿®å¤ï¼šå¤åˆ¶Taskå¯¹è±¡è€Œä¸æ˜¯å¼•ç”¨ï¼Œé¿å…ç”Ÿå‘½å‘¨æœŸé—®é¢˜
        Task task = taskOpt.value();
        if (task.type != TaskType::SEND_RESPONSE) {
            WORKER_LOGE("âŒ Invalid task type in response queue");
            continue;
        }

        ResponseTask& respTask = task.responseTask;
        processedTasks++;

        // åªè®°å½•é‡è¦çš„å“åº”äº‹ä»¶ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
        if (processedTasks % 100 == 0) {
            WORKER_LOGI("ğŸ“Š Response worker processed %{public}d tasks", processedTasks);
        }

        // ğŸ› ä¿®å¤ï¼šä¿å­˜g_sockFdå‰¯æœ¬ï¼Œé¿å…å¹¶å‘ä¿®æ”¹å¯¼è‡´çš„é—®é¢˜
        int tunnelFd = g_sockFd.load();

        // ğŸ› å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ•°æ®åŒ…æ˜¯å¦åŒ…å«IPå¤´
        // å¦‚æœæ˜¯å®Œæ•´IPåŒ…ï¼Œéœ€è¦æå–payloadï¼ˆå»æ‰IPå¤´å’Œä¼ è¾“å±‚å¤´éƒ¨ï¼‰
        const uint8_t* sendData = respTask.data;
        int sendSize = respTask.dataSize;
        int headerLen = 0;
        
        // æ£€æŸ¥æ˜¯å¦æ˜¯IPåŒ…ï¼ˆIPv4ä»¥0x4å¼€å¤´ï¼‰
        if (respTask.dataSize >= 20 && (respTask.data[0] >> 4) == 4) {
            // IPv4åŒ…ï¼šæå–payload
            int ipHeaderLen = (respTask.data[0] & 0x0F) * 4;  // IPå¤´éƒ¨é•¿åº¦
            
            if (respTask.protocol == PROTOCOL_UDP && respTask.dataSize >= ipHeaderLen + 8) {
                // UDPï¼šIPå¤´+UDPå¤´ï¼ˆ8å­—èŠ‚ï¼‰
                headerLen = ipHeaderLen + 8;
            } else if (respTask.protocol == PROTOCOL_TCP && respTask.dataSize >= ipHeaderLen + 20) {
                // TCPï¼šIPå¤´+TCPå¤´ï¼ˆè‡³å°‘20å­—èŠ‚ï¼‰
                headerLen = ipHeaderLen + 20;
            } else {
                // æœªçŸ¥åè®®æˆ–æ•°æ®åŒ…å¤ªå°ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
                headerLen = 0;
            }
            
            if (headerLen > 0 && respTask.dataSize > headerLen) {
                sendData = respTask.data + headerLen;
                sendSize = respTask.dataSize - headerLen;
                WORKER_LOGI("ğŸ”§ æå–%s payload: %{public}då­—èŠ‚ (å»æ‰%{public}då­—èŠ‚IP/ä¼ è¾“å±‚å¤´éƒ¨)", 
                           respTask.protocol == PROTOCOL_UDP ? "UDP" : "TCP",
                           sendSize, headerLen);
            } else {
                WORKER_LOGE("âš ï¸ æ— æ³•æå–payloadï¼ˆæ•°æ®åŒ…å¤ªå°æˆ–æ ¼å¼é”™è¯¯ï¼‰ï¼Œå‘é€åŸå§‹æ•°æ®");
            }
        }

        // å‘é€å“åº”ç»™å®¢æˆ·ç«¯
        if (tunnelFd >= 0 && g_running.load()) {
            ssize_t sent = sendto(tunnelFd, sendData, sendSize, 0,
                                 (struct sockaddr*)&respTask.clientAddr,
                                 sizeof(respTask.clientAddr));

            if (sent > 0) {
                responseTasksProcessed_.fetch_add(1);
                WORKER_LOGI("âœ… Response sent successfully: %{public}zd bytes to %{public}s:%{public}d", 
                           sent, 
                           inet_ntoa(respTask.clientAddr.sin_addr),
                           ntohs(respTask.clientAddr.sin_port));

                // è®¡ç®—å»¶è¿Ÿ
                auto now = std::chrono::steady_clock::now();
                auto latency = std::chrono::duration_cast<std::chrono::milliseconds>(
                    now - respTask.timestamp).count();

                if (latency > 100) {
                    WORKER_LOGI("âš ï¸ High response latency: %{public}lldms",
                               static_cast<long long>(latency));
                }
            } else {
                responseTasksFailed_.fetch_add(1);
                WORKER_LOGE("âŒ Failed to send response: errno=%{public}d (%{public}s)",
                           errno, strerror(errno));
            }
        } else {
            responseTasksFailed_.fetch_add(1);
            WORKER_LOGE("âŒ Cannot send response: tunnelFd=%{public}d, running=%{public}d",
                       tunnelFd, g_running.load());
        }
    }

    WORKER_LOGI("ğŸ”š Response worker exiting main loop");
}

WorkerThreadPool::Stats WorkerThreadPool::getStats() const {
    return {
        forwardTasksProcessed_.load(),
        responseTasksProcessed_.load(),
        forwardTasksFailed_.load(),
        responseTasksFailed_.load()
    };
}

// ========== ResponseBatcher å®ç° ==========

void ResponseBatcher::addResponse(const uint8_t* data, int dataSize,
                                  const sockaddr_in& clientAddr,
                                  int forwardSocket,
                                  uint8_t protocol) {
    std::lock_guard<std::mutex> lock(mutex_);
    
    ResponseTask task;
    if (dataSize > 0 && dataSize <= 4096) {
        std::memcpy(task.data, data, dataSize);
        task.dataSize = dataSize;
        task.clientAddr = clientAddr;
        task.forwardSocket = forwardSocket;
        task.protocol = protocol;
        task.timestamp = std::chrono::steady_clock::now();
        
        pendingResponses_.push_back(task);
    }
}

int ResponseBatcher::flush() {
    std::vector<ResponseTask> toSend;
    
    {
        std::lock_guard<std::mutex> lock(mutex_);
        toSend.swap(pendingResponses_);
    }
    
    if (toSend.empty()) {
        return 0;
    }
    
    int sent = 0;
    int sockFd = g_sockFd.load();  // ğŸ”§ ä½¿ç”¨atomicçš„load()æ–¹æ³•
    for (const auto& task : toSend) {
        if (sockFd >= 0) {
            ssize_t n = sendto(sockFd, task.data, task.dataSize, 0,
                              (struct sockaddr*)&task.clientAddr,
                              sizeof(task.clientAddr));
            if (n > 0) {
                sent++;
            }
        }
    }
    
    totalSent_.fetch_add(sent);
    totalBatches_.fetch_add(1);
    
    return sent;
}
